{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "c0cae999_ec998334",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1575067
      },
      "writtenOn": "2023-07-05T06:25:33Z",
      "side": 1,
      "message": "Hi, we upstream 4 rvv version of ScaleRowUp2.\nPlease help to review.",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "a595ceb9_0f4fa278",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1575067
      },
      "writtenOn": "2023-07-06T12:22:36Z",
      "side": 1,
      "message": "Reply to comment of extending the byte using RVV. \nOther review comments leave for Darren to reply.",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "246c77f0_a4147d04",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1115898
      },
      "writtenOn": "2023-07-06T19:02:19Z",
      "side": 1,
      "message": "Your version is okay to check in if you want to defer on optimization and/or comment changes.  I\u0027ll change the ARM version to avoid \u0027lengthen\u0027",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a0419c42_0ccdefe6",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 474,
      "author": {
        "id": 1115898
      },
      "writtenOn": "2023-07-09T08:50:46Z",
      "side": 1,
      "message": "Remove",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b01bc8f0_0b5e6f71",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 476,
      "author": {
        "id": 1115898
      },
      "writtenOn": "2023-07-09T08:50:46Z",
      "side": 1,
      "message": "src_width \u003d (dst_width - 1) \u003e\u003e 1;\nThe \u0026 ~1 is not needed if you\u0027re shifting",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "6de20dc8_937dd9dc",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 476,
      "author": {
        "id": 1573667
      },
      "writtenOn": "2023-07-09T09:48:15Z",
      "side": 1,
      "message": "Acknowledged.",
      "parentUuid": "b01bc8f0_0b5e6f71",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "d8d60bc8_08fd5171",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 481,
      "author": {
        "id": 1115898
      },
      "writtenOn": "2023-07-09T08:50:46Z",
      "side": 1,
      "message": "long term, this approach for edges prevents it from supporting clipping.  But I dont know if we\u0027ll be able to do that elegantly.\nAn example of clipping\nInstead of upsampling an entire image, do it in tiles... e.g. break it into 64x64 pixel blocks and upsample each.  The destination is 64x64 with a stride.  The source would read outside the block by 1 pixel in each direction, so 66x66\n\nAlso note that we\u0027ve implemented 4:2:0 and 4:2:2 with center of the pixels, but mpeg allows different subsample center points.  If those are ever implemented, this function may still work but get called in a different way.",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "59a965b6_10e4b7ab",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 484,
      "author": {
        "id": 1115898
      },
      "writtenOn": "2023-07-06T05:08:18Z",
      "side": 1,
      "message": "On Intel you can load bytes and extend them to 16 bits.  Does RVV have a load instruction like that?",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "577dae65_4ad63fdc",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 484,
      "author": {
        "id": 1575067
      },
      "writtenOn": "2023-07-06T12:22:36Z",
      "side": 1,
      "message": "Please refer to vector spec 11.2 Note: https://github.com/riscv/riscv-v-spec/blob/master/v-spec.adoc#112-vector-widening-integer-addsubtract\n\"An integer value can be doubled in width using the widening add instructions with a scalar operand of x0. Assembly pseudoinstructions vwcvt.x.x.v vd,vs,vm \u003d vwadd.vx vd,vs,x0,vm and vwcvtu.x.x.v vd,vs,vm \u003d vwaddu.vx vd,vs,x0,vm are provided.\"",
      "parentUuid": "59a965b6_10e4b7ab",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "ade1f739_c7fce4c8",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 484,
      "author": {
        "id": 1575067
      },
      "writtenOn": "2023-07-09T02:57:28Z",
      "side": 1,
      "message": "I misunderstood the problem. The answer is no based on my understanding.",
      "parentUuid": "577dae65_4ad63fdc",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "fa491f01_694777ac",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 484,
      "author": {
        "id": 1573667
      },
      "writtenOn": "2023-07-09T04:50:39Z",
      "side": 1,
      "message": "Yes, there\u0027s `vzext*` but after I changed add 2 here to avoid `vnclip`, I have to keep `vwaddu`:\n```\nvuint16m8_t v_src0_u16 \u003d __riscv_vwaddu_vx_u16m8(v_src0, 2, vl);\n```",
      "parentUuid": "ade1f739_c7fce4c8",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "27b3ba3e_c8187e6f",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 485,
      "author": {
        "id": 1115898
      },
      "writtenOn": "2023-07-06T05:08:18Z",
      "side": 1,
      "message": "I\u0027m not sure this +1 method of getting that second set makes sense as you get wider and wider?\nAlternative 1:\nPreload the first vector, then inside the loop, read the next vector.. of which you only need 1 byte.  Then combine the 2 vectors with an offset of 1.. on Intel this is called palignr.  On ARM it is vext.  At the bottom of the loop move the second vector to the first, or unroll the loop and toggle between 2 registers.\nAlternative 2:\nRead a full vector and then process half of it, using instructions that do paired add.  Probably not fully efficient, but on RVV you\u0027d be loading m8 and processing m4.  Or maybe load m5 and process m4?\nAlternative 3:\nRead a single byte into a second vector.  Then combine with the first vector at an offset of 1.  On ARM the 2nd load would be a load lane to read 1 byte.\nYou could shift the first vector right by 1 byte and if RVV allows it, lengthen.  And then load the 1 extra byte into the last lane of the vector.",
      "range": {
        "startLine": 485,
        "startChar": 0,
        "endLine": 485,
        "endChar": 66
      },
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "47aba7d6_3b66d694",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 485,
      "author": {
        "id": 1573667
      },
      "writtenOn": "2023-07-08T19:42:56Z",
      "side": 1,
      "message": "This pattern is bugging me ever since I see it.\nI used to apply `vslide1down` on x280 for other library functions, and the result got worse a little bit.\n\nI benchmark with this edition on p470 core here, and this time the result is very close(basically the same):\n```\nvoid ScaleRowUp2_Linear_RVV(const uint8_t* src_ptr,\n                            uint8_t* dst_ptr,\n                            int dst_width) {\n  size_t work_width \u003d ((size_t)dst_width - 1u) \u0026 ~1u;\n  size_t src_width \u003d work_width \u003e\u003e 1u;\n  const uint8_t* work_src_ptr \u003d src_ptr;\n  uint8_t* work_dst_ptr \u003d dst_ptr + 1;\n  size_t vl \u003d __riscv_vsetvl_e8m4(src_width);\n  vuint8m4_t v_3 \u003d __riscv_vmv_v_x_u8m4(3, vl);\n  dst_ptr[0] \u003d src_ptr[0];\n  while (src_width \u003e 0) {\n    vuint8m4_t v_src0, v_src1, v_dst_odd, v_dst_even;\n    vuint16m8_t v_src0_u16, v_src1_u16, v_t0_u16, v_t1_u16;\n    size_t vl \u003d __riscv_vsetvl_e8m4(src_width);\n    v_src0 \u003d __riscv_vle8_v_u8m4(work_src_ptr, vl);\n    work_src_ptr +\u003d vl;\n    v_src1 \u003d __riscv_vslide1down_vx_u8m4 (v_src0, *work_src_ptr, vl);\n\n    v_src0_u16 \u003d __riscv_vwaddu_vx_u16m8(v_src0, 2, vl);\n    v_src1_u16 \u003d __riscv_vwaddu_vx_u16m8(v_src1, 2, vl);\n    v_src0_u16 \u003d __riscv_vwmaccu_vv_u16m8(v_src0_u16, v_3, v_src1, vl);\n    v_src1_u16 \u003d __riscv_vwmaccu_vv_u16m8(v_src1_u16, v_3, v_src0, vl);\n\n    v_dst_odd \u003d __riscv_vnsrl_wx_u8m4(v_src0_u16, 2, vl);\n    v_dst_even \u003d __riscv_vnsrl_wx_u8m4(v_src1_u16, 2, vl);\n\n    __riscv_vsseg2e8_v_u8m4(work_dst_ptr, v_dst_even, v_dst_odd, vl);\n    src_width -\u003d vl;\n    work_dst_ptr +\u003d 2 * vl;\n  }\n  dst_ptr[dst_width - 1] \u003d src_ptr[(dst_width - 1) / 2];\n}\n```\n\n\nAlternative 1:\nI think we can\u0027t move the second vector to the first(toggle them).\nIn the second loop, the v_src0 needs loading from `work_src_ptr + vl`.\nIf we toggle it, the v_src0 would be from `work_src_ptr + 1`, right?\n\nAlternative 2:\nThis is interesting, although there\u0027s no m5 nor paired add, but maybe we can use m4 but not use all lanes by not setting vl to VLMAX?\n\nAlternative 3:\nThis is similar to approach mentioned above with `vslide1down`, \nBut I\u0027m not sure what `lengthen` means.\nDoes that means for instance: \nload 4 elements to v1 then append 1 element to make it hold 5 elements?\nEven if RISC-V can(by not setting vl to VLMAX), there\u0027s still lacks of paired-add.\nEventually we still need to split it into two registers(group) for later operations.",
      "parentUuid": "27b3ba3e_c8187e6f",
      "range": {
        "startLine": 485,
        "startChar": 0,
        "endLine": 485,
        "endChar": 66
      },
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "11f94e7e_5f8d177d",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 485,
      "author": {
        "id": 1575067
      },
      "writtenOn": "2023-07-09T02:57:28Z",
      "side": 1,
      "message": "\u003e v_src1 \u003d __riscv_vslide1down_vx_u8m4 (v_src0, *work_src_ptr, vl);\n\nThis looks wrong. It should be:\nv_src1 \u003d __riscv_vslide1down_vx_u8m4 (v_src0, 1, vl);",
      "parentUuid": "47aba7d6_3b66d694",
      "range": {
        "startLine": 485,
        "startChar": 0,
        "endLine": 485,
        "endChar": 66
      },
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "5507682d_0db75dec",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 485,
      "author": {
        "id": 1573667
      },
      "writtenOn": "2023-07-09T04:50:39Z",
      "side": 1,
      "message": "hmm... I use `vslide-1-down` here not `vslidedown`.",
      "parentUuid": "11f94e7e_5f8d177d",
      "range": {
        "startLine": 485,
        "startChar": 0,
        "endLine": 485,
        "endChar": 66
      },
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a52c783e_6e69d201",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 485,
      "author": {
        "id": 1115898
      },
      "writtenOn": "2023-07-09T08:50:46Z",
      "side": 1,
      "message": "Re alternative 1\nThe idea is to read full vectors source outside the loop\nThe at top of the loop read the next full set of vectors.\nThen to achieve the src+1 load effect, combine the 2 full vectors with an offset of 1 byte.  On ARM vext does this.  On Intel palignr.\n\nI do similar code to this for sobel... read a vector from a pointer and then another full vector at the same pointer + 1 and pointer - 1.  Gauss is as well.\nSo it would be good to understand the best way to handle this sort of thing.\n\nOn Intel I even go as far as re-reading the exact same pointer/vector multiple times, because the mov is fast and can run in parallel, whereas a single read and then mov has an dependency.\n\nI\u0027ve tried most of the ideas I mentioned on ARM, and the only thing that looks promising is unrolling it more, reading and writing full vectors instead of 8 bytes.\n\nslidedown is probably not super fast, but reading a very wide vector from memory has to be slower... especially if a vector is more than 1 cacheline wide.  With RVV the vector could be as little as 4 bytes, and then the multiple reads is probably fastest, but if we imagine maximum vector sizes, vector math and moves will be substantially faster than the loads and stores.\nMost libyuv functions aim to have 1 full vector store, aligned if possible, and multiple and/or unaligned reads out of order is okay.\n\nNo paired add?  hmmm.  Intel lacks a paired add but had pmaddwd or pmaddubsw which are multiple and paired add, and you can multiple by 1.  They are really dot products of 2 values.  Multiplies are fast and pipeline, so sometimes it makes sense, and I considered using multiply for this function.  Instead of\n(1 * far + 3 * near + 2) / 4\na shift by 8 can be less expensive so\n(64 * far + 192 * near + 128) / 256\nARM has addhn which is add a 16 bit value to a 16 bit value, shift right by 8 and narrow to byte.\nARM also has uzp1 which returns all the even bytes or uzp2 which returns all the odd bytes.  uzp2 takes 2 vectors and makes 1, so it can do 2x more than addhn but it doesnt do the add for rounding.\nARM has paired adds that take 2 vectors and makes 1 vector... quiet useful.\nIntel could do 64 * far + 192 * near with a vpmaddubsw but the + 128 / 256 and pack to bytes takes 3 more instructions.  I\u0027m so glad RVV is more like ARM than Intel :-)\n\nOn the ARM version of this code, its frustrating but it is already very fast, and even when I remove instructions that I suspect are slow, it stays the same speed, so if RVV is the same, I guess it is good enough, and doing the single function is a bit win.",
      "parentUuid": "5507682d_0db75dec",
      "range": {
        "startLine": 485,
        "startChar": 0,
        "endLine": 485,
        "endChar": 66
      },
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a0e4b43e_eb0f98f5",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 485,
      "author": {
        "id": 1573667
      },
      "writtenOn": "2023-07-19T17:11:06Z",
      "side": 1,
      "message": "Sorry for the late reply. I did some experiment with alternative 1:\n\nI can\u0027t find any similar `vext` or `palignr` in RVV, so I use 4 intrinsic to mimic it `vlmul_ext` + `vset_v` + `vslidedown` + `vlmul_trunc`, 3 of them are actually pseudo instructions, see the assembly there\u0027s only vslidedown (not counting vsetvli). I\u0027m aware that vslidedown might not be fast enough but using multiple (faster) instructions to assemble `vext` might not surpass single vslidedown here.\nBut the results show that there are no improvements. \n\n```\n        ...\n        vsetvli zero, a4, e8, m4, ta, ma\n        vle8.v  v12, (a7)\n        vsetvli zero, a4, e8, m8, ta, ma\n        vslidedown.vi   v16, v8, 1\n        vsetvli zero, a4, e8, m4, ta, ma\n        vwaddu.vx       v24, v8, a6\n        vwaddu.vx       v0, v16, a6\n        vwmaccu.vx      v24, t0, v16\n        vwmaccu.vx      v0, t0, v8\n        vnsrl.wi        v20, v24, 2\n        vnsrl.wi        v16, v0, 2\n        ...\n```\n\ncode:\n```\nvoid ScaleRowUp2_Linear_RVV(const uint8_t* src_ptr,\n                            uint8_t* dst_ptr,\n                            int dst_width) {\n  size_t work_width \u003d ((size_t)dst_width - 1u);\n  size_t src_width \u003d work_width \u003e\u003e 1u;\n  const uint8_t* work_src_ptr \u003d src_ptr;\n  uint8_t* work_dst_ptr \u003d dst_ptr + 1;\n  size_t vl \u003d __riscv_vsetvl_e8m4(src_width);\n  vuint8m4_t v_src0 \u003d __riscv_vle8_v_u8m4(work_src_ptr, vl);\n  dst_ptr[0] \u003d src_ptr[0];\n  work_src_ptr +\u003d vl;\n  while (src_width \u003e 0) {\n    vuint8m4_t v_src1, v_tmp, v_dst_odd, v_dst_even;\n    vuint8m8_t v_src0_ext, v_src1_tmp;\n    vuint16m8_t v_src0_u16, v_src1_u16, v_t0_u16, v_t1_u16;\n    v_tmp \u003d __riscv_vle8_v_u8m4(work_src_ptr, vl);\n    v_src0_ext \u003d __riscv_vlmul_ext_v_u8m4_u8m8 (v_src0);\n    v_src0_ext \u003d __riscv_vset_v_u8m4_u8m8 (v_src0_ext, 1, v_tmp);\n    v_src1_tmp \u003d __riscv_vslidedown_vx_u8m8 (v_src0_ext, 1, vl);\n    v_src1 \u003d __riscv_vlmul_trunc_v_u8m8_u8m4 (v_src1_tmp);\n\n    v_src0_u16 \u003d __riscv_vwaddu_vx_u16m8(v_src0, 2, vl);\n    v_src1_u16 \u003d __riscv_vwaddu_vx_u16m8(v_src1, 2, vl);\n    v_src0_u16 \u003d __riscv_vwmaccu_vx_u16m8(v_src0_u16, 3, v_src1, vl);\n    v_src1_u16 \u003d __riscv_vwmaccu_vx_u16m8(v_src1_u16, 3, v_src0, vl);\n\n    v_dst_odd \u003d __riscv_vnsrl_wx_u8m4(v_src0_u16, 2, vl);\n    v_dst_even \u003d __riscv_vnsrl_wx_u8m4(v_src1_u16, 2, vl);\n    __riscv_vsseg2e8_v_u8m4(work_dst_ptr, v_dst_even, v_dst_odd, vl);\n    v_src0 \u003d v_tmp;\n    src_width -\u003d vl;\n    work_src_ptr +\u003d vl;\n    work_dst_ptr +\u003d 2 * vl;\n    vl \u003d __riscv_vsetvl_e8m4(src_width);\n  }\n  dst_ptr[dst_width - 1] \u003d src_ptr[(dst_width - 1) / 2];\n}\n```\n\nI\u0027m planning to run some experiments to see if two vle with offset+1 will stall the pipeline.\n\nBesides addressing the load pattern in this case, I found that widening instructions are the bottleneck for this function(at least on the uarch I ran).\n\nI replaced it with few vaaddu and it got 30%+ improvement compared to the rvv implementation in patches3, though it can pass the I444Scale test with tolerance but NOT in convert_tests. Due to the rounding doing ahead in each aaddu.",
      "parentUuid": "a52c783e_6e69d201",
      "range": {
        "startLine": 485,
        "startChar": 0,
        "endLine": 485,
        "endChar": 66
      },
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "f7b67c6d_e683f622",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 485,
      "author": {
        "id": 1115898
      },
      "writtenOn": "2023-07-19T18:15:16Z",
      "side": 1,
      "message": "aaddu is an averaging add i guess.  On intel pavgb is a rounding average that is fast and I\u0027ve sometimes used that multiple times, but it causes rounding error.\nARM can do half add hadd that is the same, but it rarely makes sense, since lengthening or widening and rounding shift is faster and more accurate.\nI\u0027m not seeing great solutions here, so we should probably stick with 2 loads\n\nThe unaligned load is a technique I\u0027ve used increasingly.. it is simple and fully uses the full vectors.\nFor this especially, but also all functions, it may be interesting to artifically use smaller __riscv_vsetvl_e8m4\ne.g.  __riscv_vsetvl_e8m4(min(width, MAX_WIDTH)\nWhere MAX_WIDTH is about a cache line in size, so the processor doesnt get stuck in 2 extremely large loads\nBut that can be followup, especially when there is different hardware",
      "parentUuid": "a0e4b43e_eb0f98f5",
      "range": {
        "startLine": 485,
        "startChar": 0,
        "endLine": 485,
        "endChar": 66
      },
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "eeff75f5_f98a97e4",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 485,
      "author": {
        "id": 1573667
      },
      "writtenOn": "2023-07-21T13:18:14Z",
      "side": 1,
      "message": "I don\u0027t know about other RISC-V vender arch-specs but for platforms I benchmarked it can be cracked into multiple micro-operations (uops). (Allow me to say a little vague here.)\nFor example, hardware see `vle8_v_u8m4` will crack into 4 uops, and then after the 1st uop puts the corresponding data into the VRF, which can then be forwarded to the `vwaddu_vx_u16m8` uop. so if hw didn\u0027t run out of resources(leading vload being stalled). This helps avoid stalling due to large loads.\nBesides the vload will run ahead to fetch data into the closer buffer before vload uop is executed, also there\u0027s hw prefetcher helps bring data to cache. So I think it\u0027s ok to use a larger load here in the OOO core.\n\nBut for x280, considering this case:\n```\n    size_t vl \u003d __riscv_vsetvl_e8m4(src_width);\n    vuint8m4_t v_src0 \u003d __riscv_vle8_v_u8m4(work_src_ptr, vl); // a\n    vuint8m4_t v_src1 \u003d __riscv_vle8_v_u8m4(work_src_ptr + 1, vl); // b\n\n    vuint16m8_t v_src0_u16 \u003d __riscv_vwaddu_vx_u16m8(v_src0, 0, vl); //c\n    vuint16m8_t v_src1_u16 \u003d __riscv_vwaddu_vx_u16m8(v_src1, 0, vl); // d\n```\nThe vector command queue is a single issue queue, so after the `a` inst  \nbeing processing in load uint, the following `b`, `c`, `d` will be stuck cuz the load unit only can take one inst. So if we try to re-schedule the inst. to a c b d, if `a` put its data into VRF then it can be forwarded to c, cuz c is in the arithmetic unit(different unit from load unit). Here is the interesting part, we might think a c b d is better cuz it can be forwarded and run ahead. But after I benchmark for these kernels in libyuv, I found that most of the time it\u0027s better to put 2 loads ahead. I think it\u0027s because the following arithmetic got better pipelining if we put it this way.",
      "parentUuid": "f7b67c6d_e683f622",
      "range": {
        "startLine": 485,
        "startChar": 0,
        "endLine": 485,
        "endChar": 66
      },
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "1cc0f17c_427b38f0",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 485,
      "author": {
        "id": 1575067
      },
      "writtenOn": "2023-07-26T02:22:49Z",
      "side": 1,
      "message": "\u003e aaddu is an averaging add i guess. On intel pavgb is a rounding average that is fast and I\u0027ve sometimes used that multiple times, but it causes rounding error.\nARM can do half add hadd that is the same, but it rarely makes sense, since lengthening or widening and rounding shift is faster and more accurate.\nI\u0027m not seeing great solutions here, so we should probably stick with 2 loads\n\naaddu is averaging add. For the vector arch, there is no pair-add. \nFrom vector spec, I cannot find operations which are related to process pair elements in the same register. The reduction operations might be the one to reduce whole elements in a register to 1.",
      "parentUuid": "eeff75f5_f98a97e4",
      "range": {
        "startLine": 485,
        "startChar": 0,
        "endLine": 485,
        "endChar": 66
      },
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "db0170b6_609f9bb2",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 485,
      "author": {
        "id": 1573667
      },
      "writtenOn": "2023-07-26T03:27:52Z",
      "side": 1,
      "message": "Hi Frank, as you mentioned earlier I think we can merge this patch and defer the optimization opportunity.",
      "parentUuid": "1cc0f17c_427b38f0",
      "range": {
        "startLine": 485,
        "startChar": 0,
        "endLine": 485,
        "endChar": 66
      },
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "41da3204_d6b94703",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 489,
      "author": {
        "id": 1115898
      },
      "writtenOn": "2023-07-06T05:08:18Z",
      "side": 1,
      "message": "You\u0027ve implemented the same code as my arm, but I think my ARM code is not great.\nThe first thing I would change is make the v_3 into vuint16m8_t so you dont need to widen, and use the v_src0_u16 and v_src1_u16 that are already widened for the inputs.",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "53fcf256_718ba8aa",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 489,
      "author": {
        "id": 1573667
      },
      "writtenOn": "2023-07-08T19:42:56Z",
      "side": 1,
      "message": "The return register of `vmacc` should be used the same as the first argument(vd).\nHence, the code should be changed to:\n```\n    v_src0_u16 \u003d __riscv_vwmaccu_vv_u16m8(v_src0_u16, v_3, v_src1, vl);\n    v_src1_u16 \u003d __riscv_vwmaccu_vv_u16m8(v_src1_u16, v_3, v_src0, vl);\n```\n\nThen if we try to avoid widening we need to make a copy for both `v_src0_u16` and `v_src1_u16` before accumulating by macc.\n\notherwise, the code below is wrong:\n```\n    v_src0_u16 \u003d __riscv_vmacc_vv_u16m8(v_src0_u16, v_3_u16, v_src1_u16, vl);\n    v_src1_u16 \u003d __riscv_vmacc_vv_u16m8(v_src1_u16, v_3_u16, v_src0_u16, vl);\n```\n\nAnd making copies for these makes no gains.",
      "parentUuid": "41da3204_d6b94703",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "96f532b6_e2b8c934",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 492,
      "author": {
        "id": 1115898
      },
      "writtenOn": "2023-07-06T05:08:18Z",
      "side": 1,
      "message": "the clipping aspect is not needed for functions that interpolate.  Is there a faster instruction for shift/round/pack to 8 bits?\nOn ARM the \u0027q\u0027 in a shift narrow instruction for quantize does the clipping and is mostly free, but likely on newer cpus it generates a microop and should be simplified when possible.",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "ea4b5feb_0c89d115",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 492,
      "author": {
        "id": 1115898
      },
      "writtenOn": "2023-07-06T19:02:19Z",
      "side": 1,
      "message": "when you vwaddu the pixels, you could add 2 instead of 0, to get rounding and then use a simple shift with narrow instead of vnclipu\n vnsrl.wi vd, vs2, uimm, vm   # vector-immediate",
      "parentUuid": "96f532b6_e2b8c934",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "9a4475a7_cb87e40b",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 492,
      "author": {
        "id": 1573667
      },
      "writtenOn": "2023-07-08T19:42:56Z",
      "side": 1,
      "message": "Done.",
      "parentUuid": "ea4b5feb_0c89d115",
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c15c2410_63db26e7",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 709,
      "author": {
        "id": 1115898
      },
      "writtenOn": "2023-07-06T04:06:22Z",
      "side": 1,
      "message": "OOO threw me... suggest using XXX, (Bi)linear or [FILTER]",
      "range": {
        "startLine": 709,
        "startChar": 3,
        "endLine": 709,
        "endChar": 24
      },
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "22ff5b6b_65323473",
        "filename": "source/scale_rvv.cc",
        "patchSetId": 1
      },
      "lineNbr": 709,
      "author": {
        "id": 1573667
      },
      "writtenOn": "2023-07-08T19:42:56Z",
      "side": 1,
      "message": "Done.",
      "parentUuid": "c15c2410_63db26e7",
      "range": {
        "startLine": 709,
        "startChar": 3,
        "endLine": 709,
        "endChar": 24
      },
      "revId": "b720dcaa4224ce18d8eeeda72fd83c6b3c35e575",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    }
  ]
}